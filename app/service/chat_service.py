"""ì±„íŒ… ì„œë¹„ìŠ¤: PEFT QLoRAë¥¼ í™œìš©í•œ ëŒ€í™” ë° í•™ìŠµ.

QLoRA (Quantized LoRA): 4-bit ì–‘ìí™”ëœ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ ì ìš©í•˜ëŠ” ë°©ì‹.
ë‹¨ìˆœ ì±„íŒ…/ëŒ€í™”í˜• LLM ì¸í„°í˜ì´ìŠ¤.
ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.
"""

import os
from pathlib import Path
from typing import List, Optional, Tuple

import torch
from datasets import Dataset
from peft import LoraConfig, PeftModel, TaskType, get_peft_model, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    Trainer,
    TrainingArguments,
)

try:
    from app.service.midm_loader import DEFAULT_MIDM_PATH, load_midm  # EC2
except ImportError:
    from backend.app.service.midm_loader import DEFAULT_MIDM_PATH, load_midm  # Local

# QLoRA ì–´ëŒ‘í„° ì €ì¥ ê²½ë¡œ
DEFAULT_QLORA_ADAPTER_PATH = "backend/app/model/midm_qlora"


class ChatService:
    """PEFT QLoRAë¥¼ ì‚¬ìš©í•œ ëŒ€í™” ë° í•™ìŠµ ì„œë¹„ìŠ¤.

    QLoRAëŠ” 4-bit ì–‘ìí™”ëœ ë² ì´ìŠ¤ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ ì ìš©í•˜ì—¬
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        qlora_adapter_path: Optional[str] = None,
        use_quantization: bool = True,
        load_in_4bit: bool = True,
    ):
        """ChatService ì´ˆê¸°í™” (QLoRA ë°©ì‹).

        Args:
            model_path: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ DEFAULT_MIDM_PATH ì‚¬ìš©.
            qlora_adapter_path: QLoRA ì–´ëŒ‘í„° ê²½ë¡œ. Noneì´ë©´ DEFAULT_QLORA_ADAPTER_PATH ì‚¬ìš©.
            use_quantization: ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (QLoRAëŠ” ê¸°ë³¸ì ìœ¼ë¡œ True).
            load_in_4bit: 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (QLoRA í•„ìˆ˜).
        """
        self.model_path = model_path or DEFAULT_MIDM_PATH
        self.qlora_adapter_path = qlora_adapter_path or DEFAULT_QLORA_ADAPTER_PATH
        # QLoRAëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 4-bit ì–‘ìí™”ë¥¼ ì‚¬ìš©
        self.use_quantization = use_quantization
        self.load_in_4bit = load_in_4bit if use_quantization else False

        self.tokenizer: Optional[PreTrainedTokenizerBase] = None
        self.model: Optional[PreTrainedModel] = None
        self.peft_model: Optional[PeftModel] = None

    def _load_base_model(self) -> Tuple[PreTrainedTokenizerBase, PreTrainedModel]:
        """QLoRAìš© 4-bit ì–‘ìí™”ëœ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ."""
        if self.tokenizer is None or self.model is None:
            if self.use_quantization and self.load_in_4bit:
                # QLoRA: 4-bit ì–‘ìí™” ì„¤ì • (NF4 ì–‘ìí™” + Double Quantization)
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",  # NormalFloat4 ì–‘ìí™”
                    bnb_4bit_use_double_quant=True,  # Double Quantizationìœ¼ë¡œ ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½
                )
                print("ğŸ“¦ Loading model with QLoRA (4-bit quantization)...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True,
                    local_files_only=True,
                )
            else:
                # ì–‘ìí™” ì—†ì´ ë¡œë“œ (ì¼ë°˜ LoRA ëª¨ë“œ)
                print("âš  Loading model without quantization (standard LoRA mode)...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True,
                    local_files_only=True,
                )

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                local_files_only=True,
            )

            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

        return self.tokenizer, self.model

    def _setup_qlora(self, r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.05) -> None:
        """QLoRA ì„¤ì • ë° ëª¨ë¸ ì¤€ë¹„.

        QLoRAëŠ” 4-bit ì–‘ìí™”ëœ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
        """
        if self.model is None:
            self._load_base_model()

        # QLoRA: LoRA ì„¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=r,  # LoRA rank
            lora_alpha=lora_alpha,  # LoRA alpha
            lora_dropout=lora_dropout,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # Mi:dm ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”
            bias="none",
        )

        # QLoRA: 4-bit ì–‘ìí™”ëœ ëª¨ë¸ì„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì¤€ë¹„
        if self.use_quantization and self.load_in_4bit:
            print("ğŸ”§ Preparing 4-bit quantized model for QLoRA training...")
            self.model = prepare_model_for_kbit_training(self.model)

        # QLoRA: PEFT ëª¨ë¸ ìƒì„± (ì–‘ìí™”ëœ ë² ì´ìŠ¤ ëª¨ë¸ + LoRA ì–´ëŒ‘í„°)
        self.peft_model = get_peft_model(self.model, lora_config)
        print("ğŸ“Š Trainable parameters:")
        self.peft_model.print_trainable_parameters()

    def load_qlora_adapter(self, adapter_path: Optional[str] = None) -> None:
        """ì €ì¥ëœ QLoRA ì–´ëŒ‘í„° ë¡œë“œ."""
        adapter_path = adapter_path or self.qlora_adapter_path
        if not os.path.exists(adapter_path):
            print(f"âš  QLoRA adapter not found at {adapter_path}. Using base model.")
            self._load_base_model()
            return

        self._load_base_model()
        self.peft_model = PeftModel.from_pretrained(self.model, adapter_path)
        print(f"âœ“ QLoRA adapter loaded from {adapter_path}")

    def chat(
        self,
        message: str,
        max_new_tokens: int = 2048,  # ê¸°ë³¸ê°’ ì¦ê°€: ë” ê¸´ ì‘ë‹µ í—ˆìš©
        temperature: float = 0.7,
        top_p: float = 0.9,
        conversation_history: Optional[List[Tuple[str, str]]] = None,
    ) -> str:
        """ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€.
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜.
            temperature: ìƒì„± ì˜¨ë„.
            top_p: nucleus sampling íŒŒë¼ë¯¸í„°.
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ [(user, assistant), ...].

        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸.
        """
        if self.peft_model is None:
            # QLoRA ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            if os.path.exists(self.qlora_adapter_path):
                self.load_qlora_adapter()
            else:
                self._load_base_model()
                self._setup_qlora()

        # ëª¨ë¸ í™•ì¸
        if self.peft_model is None and self.model is None:
            raise ValueError("Model not loaded. Please initialize the model first.")

        model = self.peft_model if self.peft_model else self.model
        if model is None:
            raise ValueError("Model is None")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        if conversation_history:
            prompt = self._format_conversation(conversation_history, message)
        else:
            prompt = f"ì‚¬ìš©ì: {message}\nì–´ì‹œìŠ¤í„´íŠ¸:"

        # í† í¬ë‚˜ì´ì§• (max_length ì§€ì •ìœ¼ë¡œ ê²½ê³  í•´ê²°)
        if self.tokenizer is None:
            raise ValueError("Tokenizer not initialized")
        if model is None:
            raise ValueError("Model not initialized")

        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048,  # ì…ë ¥ ê¸¸ì´ ì œí•œ ì¦ê°€: ë” ê¸´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í—ˆìš©
        )
        # token_type_ids ì œê±° (Mi:dm ëª¨ë¸ì´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        inputs.pop("token_type_ids", None)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë” ì•ˆì „í•œ ë°©ë²•)
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì´í›„ì˜ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "ì–´ì‹œìŠ¤í„´íŠ¸:" in generated_text:
            # ë§ˆì§€ë§‰ "ì–´ì‹œìŠ¤í„´íŠ¸:" ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            parts = generated_text.split("ì–´ì‹œìŠ¤í„´íŠ¸:")
            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ì˜ "ì–´ì‹œìŠ¤í„´íŠ¸:" ì´í›„ì˜ ìƒì„±ëœ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´
            response = parts[-1].strip()
        else:
            # "ì–´ì‹œìŠ¤í„´íŠ¸:"ê°€ ì—†ìœ¼ë©´ ì „ì²´ ìƒì„±ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
            # (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°ë¥¼ ìœ„í•´ prompt ê¸¸ì´ë§Œí¼ ì œê±°)
            if prompt in generated_text:
                response = generated_text.split(prompt, 1)[-1].strip()
            else:
                response = generated_text.strip()

        return response

    def _format_conversation(self, history: List[Tuple[str, str]], current_message: str) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
        formatted = ""
        for user_msg, assistant_msg in history:
            formatted += f"ì‚¬ìš©ì: {user_msg}\nì–´ì‹œìŠ¤í„´íŠ¸: {assistant_msg}\n"
        formatted += f"ì‚¬ìš©ì: {current_message}\nì–´ì‹œìŠ¤í„´íŠ¸:"
        return formatted

    def train(
        self,
        conversations: List[List[Tuple[str, str]]],
        output_dir: Optional[str] = None,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-4,
        r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
    ) -> None:
        """ëŒ€í™” ë°ì´í„°ë¡œ QLoRA íŒŒì¸íŠœë‹.

        QLoRAëŠ” 4-bit ì–‘ìí™”ëœ ë² ì´ìŠ¤ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

        Args:
            conversations: ëŒ€í™” ë¦¬ìŠ¤íŠ¸. ê° ëŒ€í™”ëŠ” [(user, assistant), ...] í˜•ì‹.
            output_dir: í•™ìŠµëœ ì–´ëŒ‘í„° ì €ì¥ ê²½ë¡œ.
            num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜.
            batch_size: ë°°ì¹˜ í¬ê¸°.
            learning_rate: í•™ìŠµë¥ .
            r: LoRA rank.
            lora_alpha: LoRA alpha.
            lora_dropout: LoRA dropout.
        """
        output_dir = output_dir or self.qlora_adapter_path

        # QLoRA: ëª¨ë¸ ë° LoRA ì„¤ì •
        self._setup_qlora(r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)

        # ë°ì´í„°ì…‹ ì¤€ë¹„
        def format_prompt(conv: List[Tuple[str, str]]) -> str:
            """ëŒ€í™”ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
            text = ""
            for user, assistant in conv:
                text += f"ì‚¬ìš©ì: {user}\nì–´ì‹œìŠ¤í„´íŠ¸: {assistant}\n"
            return text.strip()

        texts = [format_prompt(conv) for conv in conversations]

        # í† í¬ë‚˜ì´ì§•
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=2048,  # í•™ìŠµ ì‹œ ì…ë ¥ ê¸¸ì´ ì œí•œ ì¦ê°€
                return_tensors="pt",
            )

        dataset = Dataset.from_dict({"text": texts})
        tokenized_dataset = dataset.map(tokenize_function, batched=True)

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LMì´ë¯€ë¡œ MLM ì‚¬ìš© ì•ˆ í•¨
        )

        # í•™ìŠµ ì¸ì
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=4,
            learning_rate=learning_rate,
            fp16=True,
            logging_steps=10,
            save_steps=100,
            save_total_limit=3,
            remove_unused_columns=False,
        )

        # Trainer
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )

        # QLoRA í•™ìŠµ ì‹¤í–‰
        print("ğŸš€ Starting QLoRA fine-tuning (4-bit quantized model + LoRA adapter)...")
        trainer.train()

        # QLoRA ì–´ëŒ‘í„° ì €ì¥
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        self.peft_model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print(f"âœ“ QLoRA adapter saved to {output_dir}")

    def save_adapter(self, adapter_path: Optional[str] = None) -> None:
        """í˜„ì¬ QLoRA ì–´ëŒ‘í„° ì €ì¥."""
        if self.peft_model is None:
            raise ValueError("No PEFT model loaded. Train or load an adapter first.")

        adapter_path = adapter_path or self.qlora_adapter_path
        Path(adapter_path).mkdir(parents=True, exist_ok=True)
        self.peft_model.save_pretrained(adapter_path)
        if self.tokenizer:
            self.tokenizer.save_pretrained(adapter_path)
        print(f"âœ“ QLoRA adapter saved to {adapter_path}")


# í¸ì˜ í•¨ìˆ˜
def create_chat_service(
    model_path: Optional[str] = None,
    qlora_adapter_path: Optional[str] = None,
    use_quantization: bool = True,
) -> ChatService:
    """ChatService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (QLoRA ë°©ì‹).

    Args:
        model_path: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ.
        qlora_adapter_path: QLoRA ì–´ëŒ‘í„° ê²½ë¡œ.
        use_quantization: 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (QLoRAëŠ” ê¸°ë³¸ì ìœ¼ë¡œ True).
    """
    return ChatService(
        model_path=model_path,
        qlora_adapter_path=qlora_adapter_path,
        use_quantization=use_quantization,
    )
