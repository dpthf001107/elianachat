"""LangChain RAG application with PGVector integration."""

import os
import time
from typing import List, Optional

from langchain_community.vectorstores import PGVector
from langchain_core.documents import Document
from langchain_core.embeddings import FakeEmbeddings
from langchain_core.language_models import BaseLanguageModel
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import Runnable

try:
    from app.config import settings  # type: ignore  # EC2
except ImportError:
    from backend.app.config import settings  # type: ignore  # Local

# Try to import OpenAI, fallback to fake if not available
try:
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings

    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# Database connection string from .env
CONNECTION_STRING = settings.database_url or ""

# Debug: Print DATABASE_URL status
print(f"[DEBUG] settings.database_url = {settings.database_url}")
print(f"[DEBUG] CONNECTION_STRING = {CONNECTION_STRING}")
print(f"[DEBUG] os.getenv('DATABASE_URL') = {os.getenv('DATABASE_URL')}")

# Collection name for vector store
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "langchain_collection")

# OpenAI API key (from settings, which reads from .env)
OPENAI_API_KEY = settings.openai_api_key or ""


def wait_for_postgres(max_retries: int = 30, delay: int = 2) -> None:
    """Wait for PostgreSQL (Neon DB) to be ready."""
    if not CONNECTION_STRING:
        print("âš  DATABASE_URL not set, skipping PostgreSQL connection check")
        return

    import psycopg2

    for i in range(max_retries):
        try:
            conn = psycopg2.connect(CONNECTION_STRING)
            conn.close()
            print("âœ“ PostgreSQL (Neon DB) is ready!")
            return
        except Exception as e:  # noqa: PERF203
            if i < max_retries - 1:
                print(f"Waiting for PostgreSQL (Neon DB)... ({i+1}/{max_retries})")
                time.sleep(delay)
            else:
                raise ConnectionError(f"Failed to connect to PostgreSQL (Neon DB): {e}")  # noqa: EM101


def get_embeddings():
    """Get embeddings model."""
    if HAS_OPENAI and OPENAI_API_KEY:
        print("Using OpenAI embeddings...")
        return OpenAIEmbeddings(api_key=OPENAI_API_KEY)  # type: ignore[arg-type]
    else:
        print("Using FakeEmbeddings (no API key required)...")
        return FakeEmbeddings(size=1536)


def get_llm() -> Optional[BaseLanguageModel]:
    """Get LLM model."""
    if HAS_OPENAI and OPENAI_API_KEY:
        print("Using OpenAI ChatModel...")
        # api_keyëŠ” SecretStr íƒ€ìž…ì„ ìš”êµ¬í•˜ì§€ë§Œ, strë„ ëŸ°íƒ€ìž„ì— ìž‘ë™í•¨
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì—ì„œ í•œê¸€ ì‘ë‹µì„ ê°•ì œí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ì„¤ì •ë§Œ ì‚¬ìš©
        return ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,  # ë” ìžì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìœ„í•´ temperature ì¦ê°€
            api_key=OPENAI_API_KEY,  # type: ignore[arg-type]
        )
    else:
        print("No LLM available (OpenAI API key not set). Using retrieval only mode...")
        return None


def initialize_vector_store(embeddings) -> PGVector:
    """Initialize PGVector store with sample documents."""
    if not CONNECTION_STRING:
        raise ValueError("DATABASE_URL is required for PGVector. Please set it in .env file.")

    documents = [
        Document(
            page_content="LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìžˆë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆìž„ì›Œí¬ë¡œ, ì²´ì¸ê³¼ ë„êµ¬, ì—ì´ì „íŠ¸ ë“±ì„ ì¶”ìƒí™”í•´ ì œê³µí•©ë‹ˆë‹¤.",
            metadata={"source": "langchain_intro", "topic": "framework"},
        ),
        Document(
            page_content="pgvectorëŠ” PostgreSQLì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” í™•ìž¥ìœ¼ë¡œ, ìž„ë² ë”©ì„ ì €ìž¥í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìžˆê²Œ í•´ì¤ë‹ˆë‹¤.",
            metadata={"source": "pgvector_docs", "topic": "database"},
        ),
        Document(
            page_content="RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™œìš©í•´ ë” ì •í™•í•œ ìƒì„± ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ë°©ì‹ìž…ë‹ˆë‹¤.",
            metadata={"source": "rag_concept", "topic": "ai"},
        ),
        Document(
            page_content="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ì˜ ìž„ë² ë”©ì„ ì €ìž¥í•˜ê³ , ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê°™ì€ ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
            metadata={"source": "vector_db", "topic": "database"},
        ),
        Document(
            page_content="ìž„ë² ë”©ì€ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê²ƒìœ¼ë¡œ, ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë¬¸ìž¥ì€ ì„œë¡œ ê°€ê¹Œìš´ ë²¡í„° ê°’ì„ ê°–ìŠµë‹ˆë‹¤.",
            metadata={"source": "embeddings", "topic": "ai"},
        ),
    ]

    print(f"Creating PGVector store with {len(documents)} documents...")
    vector_store = PGVector.from_documents(
        embedding=embeddings,
        documents=documents,
        collection_name=COLLECTION_NAME,
        connection_string=CONNECTION_STRING,
    )
    print("âœ“ PGVector store created!")
    return vector_store


def create_rag_chain(
    vector_store: PGVector, llm: Optional[BaseLanguageModel]
) -> Optional[Runnable]:
    """Create RAG chain using LCEL."""
    if llm is None:
        return None

    try:
        from langchain_classic.chains import create_retrieval_chain
        from langchain_classic.chains.combine_documents import create_stuff_documents_chain

        prompt_template = """ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, ì»¨í…ìŠ¤íŠ¸ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ë‹¤ê³  ë§í•  ìˆ˜ ìžˆì§€ë§Œ, ì—¬ì „ížˆ ì•Œê³  ìžˆëŠ” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì¤‘ìš”: ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {input}

ë‹µë³€ (í•œê¸€ë¡œ ìž‘ì„±):"""

        prompt = PromptTemplate.from_template(prompt_template)

        # Create document chain
        document_chain = create_stuff_documents_chain(llm, prompt)

        # Create retrieval chain
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})
        chain = create_retrieval_chain(retriever, document_chain)
        return chain
    except ImportError:
        # Fallback to deprecated API if new one is not available
        from langchain_classic.chains.retrieval_qa.base import RetrievalQA

        prompt_template = """ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, ì»¨í…ìŠ¤íŠ¸ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ë‹¤ê³  ë§í•  ìˆ˜ ìžˆì§€ë§Œ, ì—¬ì „ížˆ ì•Œê³  ìžˆëŠ” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì¤‘ìš”: ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ (í•œê¸€ë¡œ ìž‘ì„±):"""

        prompt = PromptTemplate.from_template(prompt_template)

        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True,
        )
        return chain


def interactive_mode(vector_store: PGVector, rag_chain: Optional[Runnable]) -> None:
    """Run interactive Q&A mode."""
    print("\n" + "=" * 60)
    print("Interactive Q&A Mode")
    print("=" * 60)
    print("Type your questions (or 'quit' to exit)")
    print("-" * 60)

    while True:
        try:
            query = input("\nðŸ’¬ Your question: ").strip()

            if query.lower() in ["quit", "exit", "q"]:
                print("\nðŸ‘‹ Goodbye!")
                break

            if not query:
                continue

            # If RAG chain is available, use it
            if rag_chain:
                print("\nðŸ¤” Thinking...")
                try:
                    result = rag_chain.invoke({"input": query})
                    answer = result.get("answer", result.get("result", "No answer generated"))
                    print(f"\nðŸ’¡ Answer:\n{answer}")
                    print("\nðŸ“š Sources:")
                    context = result.get("context", [])
                    if isinstance(context, list):
                        for i, doc in enumerate(context, 1):
                            if hasattr(doc, "page_content"):
                                print(f"  {i}. {doc.page_content[:100]}...")
                                print(f"     Metadata: {doc.metadata}")
                    source_docs = result.get("source_documents", [])
                    if source_docs:
                        for i, doc in enumerate(source_docs, 1):
                            print(f"  {i}. {doc.page_content[:100]}...")
                            print(f"     Metadata: {doc.metadata}")
                except Exception as e:  # noqa: BLE001
                    print(f"\nâŒ Error in RAG chain: {e}")
                    # Fallback to simple retrieval
                    fallback_results: List[Document] = vector_store.similarity_search(query, k=3)
                    print(f"\nðŸ“„ Found {len(fallback_results)} relevant documents:")
                    for i, doc in enumerate(fallback_results, 1):
                        print(f"\n  {i}. {doc.page_content}")
                        print(f"     Metadata: {doc.metadata}")
            else:
                # Fallback to simple retrieval
                print("\nðŸ” Searching...")
                results: List[Document] = vector_store.similarity_search(query, k=3)
                print(f"\nðŸ“„ Found {len(results)} relevant documents:")
                for i, doc in enumerate(results, 1):
                    print(f"\n  {i}. {doc.page_content}")
                    print(f"     Metadata: {doc.metadata}")

        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Goodbye!")
            break
        except Exception as e:  # noqa: BLE001
            print(f"\nâŒ Error: {e}")


def demo_mode(vector_store: PGVector, rag_chain: Optional[Runnable]) -> None:
    """Run demo mode with sample queries."""
    print("\n" + "=" * 60)
    print("Hello World Demo")
    print("=" * 60)

    # Sample queries
    sample_queries = [
        "What is LangChain?",
        "What is pgvector?",
        "What is RAG?",
    ]

    for query in sample_queries:
        print(f"\n{'='*60}")
        print(f"Query: {query}")
        print("=" * 60)

        # If RAG chain is available, use it
        if rag_chain:
            print("\nðŸ¤” Thinking...")
            try:
                result = rag_chain.invoke({"input": query})
                answer = result.get("answer", result.get("result", "No answer generated"))
                print(f"\nðŸ’¡ Answer:\n{answer}")
                print("\nðŸ“š Sources:")
                context = result.get("context", [])
                if isinstance(context, list):
                    for i, doc in enumerate(context, 1):
                        if hasattr(doc, "page_content"):
                            print(f"  {i}. {doc.page_content[:100]}...")
                source_docs = result.get("source_documents", [])
                if source_docs:
                    for i, doc in enumerate(source_docs, 1):
                        print(f"  {i}. {doc.page_content[:100]}...")
            except Exception as e:  # noqa: BLE001
                print(f"\nâŒ Error in RAG chain: {e}")
                # Fallback to simple retrieval
            fallback_results: List[Document] = vector_store.similarity_search(query, k=2)
            print(f"\nðŸ“„ Found {len(fallback_results)} relevant documents:")
            for i, doc in enumerate(fallback_results, 1):
                    print(f"\n  {i}. {doc.page_content}")
        else:
            # Fallback to simple retrieval
            print("\nðŸ” Searching...")
            results: List[Document] = vector_store.similarity_search(query, k=2)
            print(f"\nðŸ“„ Found {len(results)} relevant documents:")
            for i, doc in enumerate(results, 1):
                print(f"\n  {i}. {doc.page_content}")

    print("\n" + "=" * 60)
    print("Hello World Demo completed! ðŸŽ‰")
    print("=" * 60)


def main() -> None:
    """Main function to run the LangChain RAG application."""
    print("=" * 60)
    print("LangChain RAG System with PGVector")
    print("=" * 60)

    # Wait for PostgreSQL (Neon DB)
    print("\n[1/6] Checking PostgreSQL (Neon DB) connection...")
    wait_for_postgres()

    # Initialize embeddings
    print("\n[2/6] Initializing embeddings...")
    embeddings = get_embeddings()

    # Initialize LLM
    print("\n[3/6] Initializing LLM...")
    llm = get_llm()

    # Create vector store
    print("\n[4/6] Creating vector store...")
    vector_store = initialize_vector_store(embeddings)

    # Create RAG chain
    print("\n[5/6] Creating RAG chain...")
    rag_chain = create_rag_chain(vector_store, llm)
    if rag_chain:
        print("âœ“ RAG chain created!")
    else:
        print("âš  RAG chain not available (retrieval-only mode)")

    # Run demo first, then interactive mode (keeps container running)
    print("\n[6/6] Running Hello World demo...")
    demo_mode(vector_store, rag_chain)

    print("\n" + "=" * 60)
    print("Starting interactive mode...")
    print("=" * 60)
    interactive_mode(vector_store, rag_chain)


__all__ = [
    "COLLECTION_NAME",
    "CONNECTION_STRING",
    "create_rag_chain",
    "demo_mode",
    "get_embeddings",
    "get_llm",
    "initialize_vector_store",
    "interactive_mode",
    "main",
    "wait_for_postgres",
]


